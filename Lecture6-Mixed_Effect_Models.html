<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Dr. Fabio Veronesi" />


<title>Lecture 6 - Mixed Effect Model Part II</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">InferStat</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Course Summary</a>
</li>
<li>
  <a href="Lecture1_BasicR.html">Lecture 1</a>
</li>
<li>
  <a href="Lecture2_Experimental_Designs.html">Lecture 2</a>
</li>
<li>
  <a href="Lecture3-Linear_Modelling.html">Lecture 3</a>
</li>
<li>
  <a href="Lecture4-GLM.html">Lecture 4</a>
</li>
<li>
  <a href="Lecture5-Mixed_Effect_Models.html">Lecture 5</a>
</li>
<li>
  <a href="Lecture6-Mixed_Effect_Models.html">Lecture 6</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Lecture 6 - Mixed Effect Model Part II</h1>
<h4 class="author"><em>Dr. Fabio Veronesi</em></h4>
<h4 class="date"><em>April 2018</em></h4>

</div>


<div id="summary" class="section level2">
<h2><strong>Summary</strong></h2>
<ul>
<li>Dealing with Spatial and Temporal Pseudoreplication</li>
<li>Power analysis for LME</li>
</ul>
</div>
<div id="loading-packages" class="section level2">
<h2><strong>Loading Packages</strong></h2>
<pre class="r"><code>library(tidyverse)
library(car)
library(simr)
library(mvtnorm)
library(RLRsim)
library(sp)
library(gstat)
library(lmerTest)</code></pre>
<hr />
</div>
<div id="violating-the-spatial-independence-assumption" class="section level2">
<h2><strong>Violating the Spatial Independence Assumption</strong></h2>
<p>With the fertilizer dataset we looked at within-subject and temporal violations of independence. However, for studies in agriculture we are also interested in understanding how to deal with violations of independence caused by spatially correlated data. In cases where the experiment has been properly designed, with blocking to control for known sources of variation and randomization, spatial correlation should not be considered an issue. However, there are cases where we cannot avoid running experiments without proper designs. For example, when we deal with commercial companies they may not have the correct equipment to run controlled studies divided into subplot (e.g. plot seeder), so the experiment needs to be run on strips. Designing an experiment on strips causes all sorts of problems because we cannot control for external factors such as soil conditions, and to achieve good power for an ANOVA we would still need to replicate the strips a number of times, which may cause an excessive use of space. In such cases we may be forced to use pseudoreplication as a way to still achieve good power while also minimizing the use of field space.</p>
<div id="seeding-hrp-experiment" class="section level3">
<h3><strong>Seeding HRP Experiment</strong></h3>
<p>In this example (which was carried out at Harper Adams University in 2017-2018 as an HRP project by Joe Hodgson), we are interested in looking at differences between four treatments and two drilling dates on plant count. Let’s assume we approach this issue as a normal experiment we intend to analysis with linear modelling, i.e. ANOVA. To determine the number of replicates we would perform an <em>a-priori</em> power analysis, assuming a medium effect size. I ran the analysis in <a href="http://www.gpower.hhu.de/en.html">G*Power</a>, the output is below:</p>
<pre class="r"><code>dat = read.csv(&quot;Plant count data.csv&quot;, sep=&quot;;&quot;)

head(dat)</code></pre>
<pre><code>##   Week DrillingDate Plot SubPlot Treatment Count
## 1    1           D1  Pl1      P1        T1    26
## 2    1           D1  Pl1      P1        T1    36
## 3    1           D1  Pl1      P2        T1    35
## 4    1           D1  Pl1      P2        T1    29
## 5    1           D1  Pl1      P3        T1    34
## 6    1           D1  Pl1      P3        T1    33</code></pre>
<div class="figure">
<img src="Images/PowerJoe.jpg" alt="Power Analysis" />
<p class="caption">Power Analysis</p>
</div>
<p>If we assume a medium effect size (f=0.25 in the picture, d=0.5), meaning our treatment will have mean values half a standard deviation away from each other, we would need a total of 178 samples. This would clearly be a problem for any experiment, because its size would be too big to be practical. In most cases though in agriculture we have effect sizes that are larger than this, maybe around one full standard deviation of difference between treatments (f=0.5 in the picture). Therefore the sample size requirements are more manageable. The problem is that if we do not have any clue about the actual effect size for our experiment, in the literature (<a href="https://www.amazon.co.uk/Power-Analysis-Experimental-Research-Biological/dp/0521809169">Bausell &amp; Li, 2002</a>) the suggestion would be to use a medium effect size, and therefore run very large experiments.</p>
<p>For this experiment we actually had an indication of the probable effect size from a PhD thesis. On average of all the seeds treatments tested in the thesis the effect size was d=0.7, meaning almost one full standard deviation of difference. If we look back at the previous plot, this would mean an experiment with 24 strips (3 replicates). These are not very many and it would be perfectly feasible to include more blocks. However, running an experiment in strips is still not a proper design. Moreover, for this test we are assuming that the space in the field is limited, so we designed an experiment with only 8 strips, 4 for the first and 4 for the second drilling date. Below is a schematic representation of the design.</p>
<div class="figure">
<img src="Images/Exp_Design.jpeg" alt="Experimental design with pseudoreplication" />
<p class="caption">Experimental design with pseudoreplication</p>
</div>
<p>Each square is an area where plants were counted. The experiment was conducted by counting the number of plants on a weekly basis, starting on the week after emergence.</p>
<p>In a standard experiment, each area will be sampled and then results would be averaged by plot (in this case by strip). Let’s see how to simulate that in R:</p>
<pre class="r"><code>Standard.Data = dat %&gt;%
  group_by(Treatment, DrillingDate) %&gt;%
  summarise(Count = mean(Count))

Standard.Data</code></pre>
<pre><code>## # A tibble: 8 x 3
## # Groups:   Treatment [?]
##   Treatment DrillingDate Count
##   &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt;
## 1 T1        D1            35.2
## 2 T1        D2            32.8
## 3 T2        D1            31.3
## 4 T2        D2            30.4
## 5 T3        D1            31.8
## 6 T3        D2            30.2
## 7 T4        D1            29.2
## 8 T4        D2            29.1</code></pre>
<p>We have a total of 8 measurement (one for each strip). With this dataset we can apply a normal ANOVA and we will not violate any assumption (even though clearly this is not an ideal design):</p>
<pre class="r"><code>mod.st = aov(Count ~ Treatment + DrillingDate, data=Standard.Data)
summary(mod.st)</code></pre>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)  
## Treatment     3 23.841   7.947  15.943 0.0239 *
## DrillingDate  1  3.146   3.146   6.311 0.0868 .
## Residuals     3  1.495   0.498                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As you can see the omnibus ANOVA (the one that tests the null hypothesis that <span class="math inline">\(\mu_1=\mu_2=\mu_3=\mu_4\)</span>, meaning the four mean values for each treatment come from the same population) returns a significant <em>p-value</em>. I computed the <em>a-posteriori</em> power analysis using the mean square values reported above and the power of this experiment was 0.99. In other words, this experiment, albeit limited in size was able to achieve good power and reliably reject the null hypothesis, at least with a significance below 0.05. However, let’s look at the results of the Tukey multiple comparison:</p>
<pre class="r"><code>TukeyHSD(mod.st)</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Count ~ Treatment + DrillingDate, data = Standard.Data)
## 
## $Treatment
##            diff       lwr        upr     p adj
## T2-T1 -3.108333 -6.515340  0.2986732 0.0635650
## T3-T1 -2.933333 -6.340340  0.4736732 0.0737420
## T4-T1 -4.800000 -8.207007 -1.3929935 0.0195654
## T3-T2  0.175000 -3.232007  3.5820065 0.9935792
## T4-T2 -1.691667 -5.098673  1.7153398 0.2561146
## T4-T3 -1.866667 -5.273673  1.5403398 0.2105209
## 
## $DrillingDate
##            diff       lwr       upr     p adj
## D2-D1 -1.254167 -2.842938 0.3346051 0.0867692</code></pre>
<p>From this table it is clear that among all contrasts, the only significant difference was detected between <code>T1</code> and <code>T4</code>. Let’s compute the effect size (d, standardized mean differences) to better understand the real power of the experiment:</p>
<pre class="r"><code>ES.Data = as.data.frame(Standard.Data)
Control = ES.Data[ES.Data$Treatment==&quot;T4&quot;,&quot;Count&quot;]
Treatment1 = ES.Data[ES.Data$Treatment==&quot;T1&quot;,&quot;Count&quot;]

numerator = (mean(Treatment1)-mean(Control))
denominator = sqrt((((length(Treatment1)-1)*sd(Treatment1)^2)+((length(Control)-1)*sd(Control)^2))/(length(Treatment1)+length(Control)-2))

numerator/denominator</code></pre>
<pre><code>## [1] 3.943726</code></pre>
<p>The only difference that the model above was capable of detecting was two mean values separated by 4 standard deviations. However, if we look at the effect size between <code>T1</code> and <code>T2</code>:</p>
<pre class="r"><code>Control = ES.Data[ES.Data$Treatment==&quot;T2&quot;,&quot;Count&quot;]
Treatment1 = ES.Data[ES.Data$Treatment==&quot;T1&quot;,&quot;Count&quot;]

numerator = (mean(Treatment1)-mean(Control))
denominator = sqrt((((length(Treatment1)-1)*sd(Treatment1)^2)+((length(Control)-1)*sd(Control)^2))/(length(Treatment1)+length(Control)-2))

numerator/denominator</code></pre>
<pre><code>## [1] 2.379855</code></pre>
<p>We can clearly see that even this difference is quite large. In other words, the experiment we ran above was powered only to reliably distinguish between mean values separated by four standard deviation, which is massive effect size, that would be clearly detectable with a naked eye in the field, so probably there would be no need to run a formal test.</p>
<p>Let’s now try to analyse this dataset as a repeated measure experiment. We can average values by plot using the following lines:</p>
<pre class="r"><code>Temporal.Data = dat %&gt;%
  group_by(Treatment, DrillingDate, Week, Plot) %&gt;%
  summarise(Count = mean(Count))

Temporal.Data</code></pre>
<pre><code>## # A tibble: 40 x 5
## # Groups:   Treatment, DrillingDate, Week [?]
##    Treatment DrillingDate  Week Plot  Count
##    &lt;fct&gt;     &lt;fct&gt;        &lt;int&gt; &lt;fct&gt; &lt;dbl&gt;
##  1 T1        D1               1 Pl1    35.9
##  2 T1        D1               2 Pl1    35.8
##  3 T1        D1               3 Pl1    35.0
##  4 T1        D1               4 Pl1    34.4
##  5 T1        D1               5 Pl1    34.8
##  6 T1        D2               1 Pl5    23.4
##  7 T1        D2               2 Pl5    32.8
##  8 T1        D2               3 Pl5    35.3
##  9 T1        D2               4 Pl5    36.1
## 10 T1        D2               5 Pl5    36.1
## # ... with 30 more rows</code></pre>
<p>Now we can use the function <code>lme</code> to fit a mixed effect model:</p>
<pre class="r"><code>mod.tp = lme(Count ~ Treatment + DrillingDate, random = ~ 1|Plot, data=Temporal.Data, method=&quot;REML&quot;)
summary(mod.tp)</code></pre>
<pre><code>## Linear mixed-effects model fit by REML
##  Data: Temporal.Data 
##       AIC      BIC    logLik
##   198.052 208.9395 -92.02602
## 
## Random effects:
##  Formula: ~1 | Plot
##          (Intercept) Residual
## StdDev: 0.0001308318 2.845976
## 
## Fixed effects: Count ~ Treatment + DrillingDate 
##                   Value Std.Error DF  t-value p-value
## (Intercept)    34.59375 1.0062044 32 34.38044  0.0000
## TreatmentT2    -3.10833 1.2727591  3 -2.44220  0.0923
## TreatmentT3    -2.93333 1.2727591  3 -2.30470  0.1045
## TreatmentT4    -4.80000 1.2727591  3 -3.77133  0.0326
## DrillingDateD2 -1.25417 0.8999766  3 -1.39355  0.2577
##  Correlation: 
##                (Intr) TrtmT2 TrtmT3 TrtmT4
## TreatmentT2    -0.632                     
## TreatmentT3    -0.632  0.500              
## TreatmentT4    -0.632  0.500  0.500       
## DrillingDateD2 -0.447  0.000  0.000  0.000
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -3.4866483 -0.2100920  0.1134643  0.4791122  1.3798723 
## 
## Number of Observations: 40
## Number of Groups: 8</code></pre>
<p>The summary tables tells us that <code>Plot</code> can explain very little of the variance in the dataset, and the <em>p-values</em> are still only significant between <code>T1</code> (reference level not shown) and <code>T4</code>. I ran a power analysis that concluded we needed at least 12 measurement (in total) to detect the difference between <code>T1</code> and <code>T2</code>, despite it being very large.</p>
<p>Drilling date also appears not significant. We can again run a power analysis to understand the average differences between drilling dates:</p>
<pre class="r"><code>ES.Data = as.data.frame(Temporal.Data)
Control = ES.Data[ES.Data$DrillingDate==&quot;D2&quot;,&quot;Count&quot;]
Treatment1 = ES.Data[ES.Data$DrillingDate==&quot;D1&quot;,&quot;Count&quot;]

numerator = (mean(Treatment1)-mean(Control))
denominator = sqrt((((length(Treatment1)-1)*sd(Treatment1)^2)+((length(Control)-1)*sd(Control)^2))/(length(Treatment1)+length(Control)-2))

numerator/denominator</code></pre>
<pre><code>## [1] 0.3852655</code></pre>
<p>In this case average differences between drilling dates are relatively small, less than half a standard deviation, which is clearly too small to be detectable with such a small experiment. However, let’s what happens if we include the pseudoreplicate. For this model we need to use the package <code>lme4</code> because its optimisation algorithm is better for complex models:</p>
<pre class="r"><code>full.lme = lmer(Count ~ Treatment + DrillingDate + (factor(Week)|Plot), data=dat)

summary(full.lme)</code></pre>
<pre><code>## Linear mixed model fit by REML t-tests use Satterthwaite approximations
##   to degrees of freedom [lmerMod]
## Formula: Count ~ Treatment + DrillingDate + (factor(Week) | Plot)
##    Data: dat
## 
## REML criterion at convergence: 2613.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.0890 -0.5006  0.0090  0.5936  3.7554 
## 
## Random effects:
##  Groups   Name          Variance Std.Dev. Corr                   
##  Plot     (Intercept)   41.91    6.474                           
##           factor(Week)2 30.21    5.496    -0.99                  
##           factor(Week)3 45.07    6.714    -1.00  1.00            
##           factor(Week)4 38.84    6.232    -1.00  0.99  1.00      
##           factor(Week)5 38.04    6.168    -1.00  0.99  1.00  1.00
##  Residual               12.76    3.572                           
## Number of obs: 480, groups:  Plot, 8
## 
## Fixed effects:
##                Estimate Std. Error      df t value Pr(&gt;|t|)    
## (Intercept)     34.8301     0.4339 22.8876  80.281  &lt; 2e-16 ***
## TreatmentT2     -3.6376     0.5488 22.8876  -6.629 9.43e-07 ***
## TreatmentT3     -3.2506     0.5488 22.8876  -5.923 4.97e-06 ***
## TreatmentT4     -5.3814     0.5488 22.8876  -9.806 1.16e-09 ***
## DrillingDateD2   0.9583     0.3881 22.8876   2.470   0.0214 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) TrtmT2 TrtmT3 TrtmT4
## TreatmentT2 -0.632                     
## TreatmentT3 -0.632  0.500              
## TreatmentT4 -0.632  0.500  0.500       
## DrillngDtD2 -0.447  0.000  0.000  0.000</code></pre>
<p>Now the random effects can explain a lot of the variance in the data, and the <em>p-values</em> are all significant, meaning that this model can detect difference between <code>T1</code> (reference level not shown) and all the other. This model also reports a significant <em>p-value</em> for drilling date <code>D2</code>, but we will see later on that the power for detecting such a small effect size is still low.</p>
</div>
<div id="post-hoc-power-analysis" class="section level3">
<h3><strong><em>Post Hoc</em> Power Analysis</strong></h3>
<p>To compute the power of this model we can use the function <code>powerSim</code> in the package <code>simr</code>. Since it is based on a simulation it takes a while to compute the results, and the more complex is the model to slowest the computation will become:</p>
<pre class="r"><code>powerSim(full.lme, fixed(&quot;Treatment&quot;, &quot;lr&quot;), nsim=50, progress=F)</code></pre>
<pre><code>## Power for predictor &#39;Treatment&#39;, (95% confidence interval):
##       98.00% (89.35, 99.95)
## 
## Test: Likelihood ratio
## 
## Based on 50 simulations, (18 warnings, 0 errors)
## alpha = 0.05, nrow = 480
## 
## Time elapsed: 0 h 0 m 50 s
## 
## nb: result might be an observed power calculation</code></pre>
<p>Clearly this model has full power to detect the effect of treatment. We can compute power to detect specific contrasts using the following line:</p>
<pre class="r"><code>powerSim(full.lme, fixed(&quot;TreatmentT2&quot;, &quot;z&quot;), nsim=50, progress=F)</code></pre>
<pre><code>## Power for predictor &#39;TreatmentT2&#39;, (95% confidence interval):
##       100.0% (92.89, 100.0)
## 
## Test: z-test
##       Effect size for TreatmentT2 is -3.6
## 
## Based on 50 simulations, (14 warnings, 0 errors)
## alpha = 0.05, nrow = 480
## 
## Time elapsed: 0 h 0 m 17 s
## 
## nb: result might be an observed power calculation</code></pre>
<p>Again, clearly this model is more powerful than what we fitted before. Now we have enough power to detect differences between <code>T1</code> and <code>T2</code>. To check the power for drilling date we can use the following line:</p>
<pre class="r"><code>powerSim(full.lme, fixed(&quot;DrillingDate&quot;), nsim=50, progress=F)</code></pre>
<pre><code>## Power for predictor &#39;DrillingDate&#39;, (95% confidence interval):
##       50.00% (35.53, 64.47)
## 
## Test: Likelihood ratio
## 
## Based on 50 simulations, (20 warnings, 0 errors)
## alpha = 0.05, nrow = 480
## 
## Time elapsed: 0 h 0 m 48 s
## 
## nb: result might be an observed power calculation</code></pre>
<p>As you can see the power to detect drilling date is very low, so we cannot really accept the significant value we obtained above. This model is still not powerful enough to detect such small differences.</p>
</div>
<div id="including-variogram" class="section level3">
<h3><strong>Including Variogram</strong></h3>
<p>If it is expected to have data that are spatially correlated, it is good practice to record the exact geographic location of each sample with GPS. This allows us to compute the variogram and obtain the spatial autocorrelation structure of the study area. This can be included directly into the model.</p>
<p>Let’s look at an example:</p>
<pre class="r"><code>Boreality = read.table(url(&quot;https://raw.githubusercontent.com/jebyrnes/spatial_correction_lavaan/master/Boreality.txt&quot;), sep=&quot;&quot;, header=T)

head(Boreality)</code></pre>
<pre><code>##   point       x       y Oxalis   boreal nBor nTot       Grn     NDVI
## 1     1 2109.70 2093.52      0 15.38462    2   13 0.0597027 0.480180
## 2     2 2190.18 2105.71      1 19.04762    4   21 0.0514881 0.483990
## 3     3 2064.48 2052.77      1 20.00000    6   30 0.0509510 0.489213
## 4     4 2277.34 2103.42      0 15.38462    2   13 0.0521183 0.473226
## 5     5 2347.91 2074.81      0  0.00000    0   13 0.0422267 0.405898
## 6     6 2437.21 2086.95      0 16.66667    1    6 0.0417779 0.424769
##       T61        Wet
## 1 296.367 -0.0264378
## 2 296.367 -0.0234048
## 3 296.367 -0.0189264
## 4 296.367 -0.0280431
## 5 296.785 -0.0292287
## 6 296.367 -0.0229209</code></pre>
<p>This dataset contains more than 500 spatial observations of several properties.</p>
<pre class="r"><code>coordinates(Boreality) = ~ x + y</code></pre>
<p>We can look at the variogram with the line below:</p>
<pre class="r"><code>VarOmni = variogram(boreal ~ 1, data=Boreality)
plot(VarOmni)</code></pre>
<p><img src="Lecture6-Mixed_Effect_Models_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>To determine the values to include in the option <code>corSpher</code> we need to create a variogram of the residuals:</p>
<pre class="r"><code>B1.gls &lt;- gls(boreal ~ 1, data = Boreality)

var1 &lt;- Variogram(B1.gls ,form=~x+y)

plot(var1,smooth=T)</code></pre>
<p><img src="Lecture6-Mixed_Effect_Models_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Now we have a nugget (semivariance value at distance equal to 0) and range (distance at which the variograms flattens):</p>
<pre class="r"><code>lme = gls(boreal ~ Wet, data=Boreality)
sp.lme = gls(boreal ~ Wet, data=Boreality, correlation = corSpher(value=c(2000, 0.5),
                                                                  form = ~ x + y, 
                                                                  nugget=T))

anova(lme, sp.lme)</code></pre>
<pre><code>##        Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## lme        1  3 3855.122 3867.946 -1924.561                        
## sp.lme     2  5 3742.106 3763.480 -1866.053 1 vs 2 117.0153  &lt;.0001</code></pre>
<p>The output clearly shows that the spatial model is more accurate.</p>
<hr />
</div>
</div>
<div id="a-priori-power-analysis-for-mixed-effect-models" class="section level2">
<h2><strong><em>A Priori</em> Power Analysis for Mixed Effect Models</strong></h2>
<p>The only way to perform an <em>a-priori</em> power analysis for mixed effect models is to run a simulation. The procedure below is a bit cumbersome and very experimental, but for the time being this seems to be the only plausible way of performing an _a-priori_power analysis for LME.</p>
<p>In this first example we explore how to perform a power analysis for repeated measures experiments in agriculture. I am not covering animal studies because most of the time latin-square that have particular rules of design (like 3x3 or 4x4 squares).</p>
<div id="create-a-dummy-dataset" class="section level3">
<h3><strong>Create a Dummy Dataset</strong></h3>
<p>Let’s say we want to perform a power analysis for an experiment with four treatments. This may be a one-way ANOVA, but it could also be an experiment with more than one treatment level. However, for simplicity it is better if we include all treatment combinations into a single column, the same way the package <code>agricolae</code> creates experimental designs. This will make the power analysis a bit easier.</p>
<p>The first step is to create a <code>data.frame</code> with the full treatment structure using the function <code>expand.grid</code>. Here we can specify the number of replicates and the number of repeated measures:</p>
<pre class="r"><code>Design = expand.grid(Replicate = 1:3,
                     Week = 1:5,
                     Treatment = c(&quot;Tr1&quot;, &quot;Tr2&quot;, &quot;Tr3&quot;, &quot;Tr4&quot;))

str(Design)</code></pre>
<pre><code>## &#39;data.frame&#39;:    60 obs. of  3 variables:
##  $ Replicate: int  1 2 3 1 2 3 1 2 3 1 ...
##  $ Week     : int  1 1 1 2 2 2 3 3 3 4 ...
##  $ Treatment: Factor w/ 4 levels &quot;Tr1&quot;,&quot;Tr2&quot;,&quot;Tr3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  - attr(*, &quot;out.attrs&quot;)=List of 2
##   ..$ dim     : Named int  3 5 4
##   .. ..- attr(*, &quot;names&quot;)= chr  &quot;Replicate&quot; &quot;Week&quot; &quot;Treatment&quot;
##   ..$ dimnames:List of 3
##   .. ..$ Replicate: chr  &quot;Replicate=1&quot; &quot;Replicate=2&quot; &quot;Replicate=3&quot;
##   .. ..$ Week     : chr  &quot;Week=1&quot; &quot;Week=2&quot; &quot;Week=3&quot; &quot;Week=4&quot; ...
##   .. ..$ Treatment: chr  &quot;Treatment=Tr1&quot; &quot;Treatment=Tr2&quot; &quot;Treatment=Tr3&quot; &quot;Treatment=Tr4&quot;</code></pre>
</div>
<div id="create-the-variancecovariance-matrix" class="section level3">
<h3><strong>Create the Variance/Covariance Matrix</strong></h3>
<p>We need to specify the standard deviation of each group, we can select 1 for simplicity. The actual value does not really matter, what matters is the effect size between groups which we need to see at d=0.5. However, if you have previous observations you could modify this value to obtain a dummy dataset more in line with what you expect in the field.</p>
<pre class="r"><code>sigma = 1 </code></pre>
<p>The next parameter to set is the correlation between repeated measures, which we can set at 0.5 as suggested in Faul et al. (<a href="http://www.gpower.hhu.de/fileadmin/redaktion/Fakultaeten/Mathematisch-Naturwissenschaftliche_Fakultaet/Psychologie/AAP/gpower/GPower3-BRM-Paper.pdf">2007</a>, pag.181):</p>
<pre class="r"><code>rho = 0.5  </code></pre>
<p>Finally we can set the number of repeated measures (5 weeks):</p>
<pre class="r"><code>n.rep.measures = 5</code></pre>
<p>Now we have all the elements to create the variance/covariance matrix with the following code:</p>
<pre class="r"><code>sigma.mat &lt;- rep(sigma, n.rep.measures)  
S &lt;- matrix(sigma.mat, ncol=length(sigma.mat), nrow=length(sigma.mat))  
Sigma &lt;- t(S) * S * rho   
diag(Sigma) &lt;- sigma^2 

Sigma</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]  1.0  0.5  0.5  0.5  0.5
## [2,]  0.5  1.0  0.5  0.5  0.5
## [3,]  0.5  0.5  1.0  0.5  0.5
## [4,]  0.5  0.5  0.5  1.0  0.5
## [5,]  0.5  0.5  0.5  0.5  1.0</code></pre>
</div>
<div id="fill-the-dummy-dataset" class="section level3">
<h3><strong>Fill the Dummy Dataset</strong></h3>
<p>In literature the suggestion is to compute sample size based on a medium effect size (d=0.5), so we need to remember to specify differences between treatment levels of half a standard deviation. Since we set the standard deviation of each group at 1, the differences will be 0.5.</p>
<p>The procedure is a bit long. We can use the function <code>rmvnorm</code> to create random samples with particular correlations (in this case 0.5). This function creates several columns, each taken from a normal distribution with standard deviation of 1, and mean values that we can specify in the call to the function. The mean values are referred to the repeated measures. In this case we are assuming a linear increase in mean values with time, so we start from a mean of 20 (we can choose whatever value we want) and we increase the other means by 1:</p>
<pre class="r"><code>Tr1 = rmvnorm(n = 3, 
              mean = c(20, 21, 22, 23, 24),
              sigma = Sigma)

Tr1</code></pre>
<pre><code>##          [,1]     [,2]     [,3]     [,4]     [,5]
## [1,] 20.37304 22.25325 23.79948 23.80150 24.86301
## [2,] 20.24320 19.62282 22.12461 22.55495 23.91073
## [3,] 19.69667 21.85445 24.13568 23.00728 25.00216</code></pre>
<p>Each column has n=3 values (since 3 are the number of replicates in the field or the number of subjects), taken from normal distribution with known means and standard deviation of 1.</p>
<p>To create the values for the other treatments we need to remember</p>
<pre class="r"><code>Tr2 = rmvnorm(n = 3, 
              mean = c(20.5, 21.5, 22.5, 23.5, 24.5),
              sigma = Sigma)

Tr3 = rmvnorm(n = 3, 
              mean = c(21, 22, 23, 24, 25),
              sigma = Sigma)

Tr4 = rmvnorm(n = 3, 
              mean = c(21.5, 22.5, 23.5, 24.5, 25.5),
              sigma = Sigma)</code></pre>
<p>Now we need to fill the <code>data.frame</code> with these new simulated values. First of all we need to create a new column names <code>Response</code> where we can include the values we simulated:</p>
<pre class="r"><code>Design$Response = 1:nrow(Design)</code></pre>
<p>Now we need to subset the dataset and fill the new column. Let’s look at one of these subsets:</p>
<pre class="r"><code>Design[Design$Treatment==&quot;Tr1&quot;,]</code></pre>
<pre><code>##    Replicate Week Treatment Response
## 1          1    1       Tr1        1
## 2          2    1       Tr1        2
## 3          3    1       Tr1        3
## 4          1    2       Tr1        4
## 5          2    2       Tr1        5
## 6          3    2       Tr1        6
## 7          1    3       Tr1        7
## 8          2    3       Tr1        8
## 9          3    3       Tr1        9
## 10         1    4       Tr1       10
## 11         2    4       Tr1       11
## 12         3    4       Tr1       12
## 13         1    5       Tr1       13
## 14         2    5       Tr1       14
## 15         3    5       Tr1       15</code></pre>
<p>Since the repeated measures are ordered (i.e. the first three measures per <code>Week</code> are one after the other), we can use the object we created above to fill the entire subset easily:</p>
<pre class="r"><code>Design[Design$Treatment==&quot;Tr1&quot;,&quot;Response&quot;] = c(Tr1[,1], Tr1[,2], Tr1[,3], Tr1[,4], Tr1[,5])</code></pre>
<p>Then we need to do the same for the other treatment levels:</p>
<pre class="r"><code>Design[Design$Treatment==&quot;Tr2&quot;,&quot;Response&quot;] = c(Tr2[,1], Tr2[,2], Tr2[,3], Tr2[,4], Tr2[,5])
Design[Design$Treatment==&quot;Tr3&quot;,&quot;Response&quot;] = c(Tr3[,1], Tr3[,2], Tr3[,3], Tr3[,4], Tr3[,5])
Design[Design$Treatment==&quot;Tr4&quot;,&quot;Response&quot;] = c(Tr4[,1], Tr4[,2], Tr4[,3], Tr4[,4], Tr4[,5])</code></pre>
<p>Let’s look at the object <code>Design</code> now:</p>
<pre class="r"><code>head(Design)</code></pre>
<pre><code>##   Replicate Week Treatment Response
## 1         1    1       Tr1 20.37304
## 2         2    1       Tr1 20.24320
## 3         3    1       Tr1 19.69667
## 4         1    2       Tr1 22.25325
## 5         2    2       Tr1 19.62282
## 6         3    2       Tr1 21.85445</code></pre>
</div>
<div id="fitting-a-model" class="section level3">
<h3><strong>Fitting a Model</strong></h3>
<p>Now that we have a dataset with the correct effect size between groups and the correct correlation between repeated measures we can fit an LME model and check its power:</p>
<pre class="r"><code>Design$Week = as.factor(Design$Week)

sim.model = lmer(Response ~  Treatment + (1|Week), data=Design)

summary(sim.model)</code></pre>
<pre><code>## Linear mixed model fit by REML t-tests use Satterthwaite approximations
##   to degrees of freedom [lmerMod]
## Formula: Response ~ Treatment + (1 | Week)
##    Data: Design
## 
## REML criterion at convergence: 181.3
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.05389 -0.65458  0.08337  0.61673  1.93960 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Week     (Intercept) 2.2757   1.5086  
##  Residual             0.9658   0.9827  
## Number of obs: 60, groups:  Week, 5
## 
## Fixed effects:
##              Estimate Std. Error      df t value Pr(&gt;|t|)    
## (Intercept)   22.4829     0.7208  4.8578  31.192 8.74e-07 ***
## TreatmentTr2   0.8311     0.3588 52.0000   2.316   0.0245 *  
## TreatmentTr3  -0.1741     0.3588 52.0000  -0.485   0.6295    
## TreatmentTr4   0.8194     0.3588 52.0000   2.283   0.0265 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) TrtmT2 TrtmT3
## TreatmntTr2 -0.249              
## TreatmntTr3 -0.249  0.500       
## TreatmntTr4 -0.249  0.500  0.500</code></pre>
<p>Now we can check the power of this model as we did before (nsim is used to reduce the computational time during testing, to get a more robust measure it is better not to include this option):</p>
<pre class="r"><code>powerSim(sim.model, nsim=10, progress=F)</code></pre>
<pre><code>## Power for predictor &#39;Treatment&#39;, (95% confidence interval):
##       100.0% (69.15, 100.0)
## 
## Test: Likelihood ratio
## 
## Based on 10 simulations, (0 warnings, 0 errors)
## alpha = 0.05, nrow = 60
## 
## Time elapsed: 0 h 0 m 0 s
## 
## nb: result might be an observed power calculation</code></pre>
<p>The power seems to be good, even though the lower bound of the simulation is only 69.15%. Again we can check the power to detect the specific effect size (d=0.5) including an option to test the differences between <code>Tr1</code> and <code>Tr2</code>:</p>
<pre class="r"><code>powerSim(sim.model, fixed(&quot;TreatmentTr2&quot;, &quot;z&quot;), nsim=10, progress=F)</code></pre>
<pre><code>## Power for predictor &#39;TreatmentTr2&#39;, (95% confidence interval):
##       70.00% (34.75, 93.33)
## 
## Test: z-test
##       Effect size for TreatmentTr2 is 0.83
## 
## Based on 10 simulations, (0 warnings, 0 errors)
## alpha = 0.05, nrow = 60
## 
## Time elapsed: 0 h 0 m 0 s
## 
## nb: result might be an observed power calculation</code></pre>
<p>It seems clear that this setting is not powerful enough to detect medium effect sizes, so probably it would be a good idea to repeat the experiment with more replicates.</p>
</div>
</div>

<p>Copyright &copy; 2018 Dr. Fabio Veronesi - Creative Commons Attribution 3.0 Unported</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
